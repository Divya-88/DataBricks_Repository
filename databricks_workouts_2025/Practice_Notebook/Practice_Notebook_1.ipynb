{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "b553000e-ee88-4902-b9ec-28fc3747e579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf1)\n",
    "display(rawdf2)\n",
    "rawdf_merged=rawdf1.unionByName(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype..\n",
    "display(rawdf_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3158c03-4c7e-4ab3-8593-bd50922ac675",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/sple_10000_records.csv\",\"/Volumes/workspace/wd36schema/ingestion_volume/source/custsmodified\"])\n",
    "\n",
    "display(rawdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae65716d-3f02-4ae3-8926-381efcb97511",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "strct1= \"productId string, productName  string, stdCost double, stdPrice double, effDt timestamp, dt date\"\n",
    "df1=spark.read.schema(strct1).csv(path=\"/Volumes/workspace/wd36schema/ingestion_volume/sales.csv\",sep=\",\",header=True, multiLine=True, escape=\"~\", comment='#', ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=True)\n",
    "print(df1.schema)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d16855dd-6682-4704-bc4c-2b8b49c6bc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=True,inferSchema=True,sep=\",\")\n",
    "display(df_cust)\n",
    "\n",
    "df_cust.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dcb3061a-afc8-475d-99ae-6fd7ae8d909c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE telecom_catalog_assign.landing_zone.customer_managed1 (\n",
    "    customer_id INT,\n",
    "    name STRING,\n",
    "    age INT,\n",
    "    city STRING,\n",
    "    plan STRING\n",
    ")\n",
    "USING DELTA;\n",
    "\n",
    "SELECT * FROM telecom_catalog_assign.landing_zone.customer_managed1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d418a04-53bb-4dc5-a82e-ea9281932fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", header=True, inferSchema=True).toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "df_cust.write.saveAsTable(\"telecom_catalog_assign.landing_zone.customer_managed2\")\n",
    "df_cust.write.insertInto(\n",
    "    \"telecom_catalog_assign.landing_zone.customer_managed2\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "display(spark.sql(\"SELECT * FROM telecom_catalog_assign.landing_zone.customer_managed2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ce9af3b-13b5-4f9f-a941-f214cccf73d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE telecom_catalog_assign.landing_zone.customer_managed;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1fb8e9-63fa-4084-95ad-80711a4753ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1: Write customer data using insertInto() in a new table and find the behavior\n",
    "#table created for customer\n",
    "read_customer_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", sep=\",\").toDF(\"id\",\"name\",\"age\",\"city\",\"sim_type\") \n",
    "read_customer_df.write.saveAsTable(\"telecom_catalog_assign.landing_zone.custtbl\",mode='overwrite')\n",
    "#insert into customer table already created\n",
    "dlt_tb = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custs.csv\").toDF(\"id\", \"name\", \"age\", \"city\", \"sim_type\")\n",
    "dlt_tb.write.insertInto(\"telecom_catalog_assign.landing_zone.custtbl\")\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.custtbl\"))\n",
    "\n",
    "#2:Write usage data using insertTable() with overwrite mode\n",
    "read_usage = spark.read.options(header=\"True\",sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_new.csv\") \n",
    "read_usage.write.insertInto(\"telecom_catalog_assign.landing_zone.usagetbl\",overwrite=True)\n",
    "spark.sql(\"select * from telecom_catalog_assign.landing_zone.usagetbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fdb596d-9a2d-44de-aa76-fd9e0f1f0aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaOverwriteExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (108, \"John\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (109,\"Jane\",20,\"Delhi\",\"PREPAID\"),\n",
    "    (110,\"Alice\",25,\"Pune\",\"POSTPAID\"),\n",
    "    (111,\"Bob\",30,\"Bangalore\",\"PREPAID\"),\n",
    "\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"city\", \"plan\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Path to Delta table\n",
    "delta_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/\"\n",
    "\n",
    "# Write DataFrame in Delta format using overwrite mode\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "spark.read.format(\"delta\").load(delta_path).count()\n",
    "\n",
    "print(\"Data written to Delta table with overwrite mode successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be313fb5-5ca1-4ea7-a702-988c82297cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, \"John\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (103,\"Jane\",20,\"Delhi\",\"PREPAID\"),\n",
    "    (102,\"Alice\",25,\"Pune\",\"POSTPAID\"),\n",
    "    (107,\"Bob\",30,\"Bangalore\",\"PREPAID\"),\n",
    "    (106,\"Bob\",30,\"Bangalore\",\"PREPAID\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"city\", \"plan\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer_practice/\")\n",
    "df1 = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer_practice/\")\n",
    "display(df1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5929b88-468f-4dcc-a47b-fae93bb4e632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaOverwriteExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (108, \"John\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (109,\"Jane\",20,\"Delhi\",\"PREPAID\"),\n",
    "    (110,\"Alice\",25,\"Pune\",\"POSTPAID\"),\n",
    "    (111,\"Bob\",30,\"Bangalore\",\"PREPAID\"),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"amt\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Path to Delta table\n",
    "delta_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\"\n",
    "\n",
    "# Write DataFrame in Delta format using overwrite mode\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# print(\"Data written to Delta table with overwrite mode successfully.\")\n",
    "# df.write.format(\"delta\").option(\"mergeSchema\", \"true\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .save(delta_path)\n",
    "\n",
    "print(\"Data written to Delta table with overwrite mode successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cf8bf2-f95f-4915-90d4-af96b81f1423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/X.c000.snappy.parquet\")\n",
    "# df1.show()\n",
    "df1.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/Y.c000.snappy.parquet\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c3703a-eed8-4df5-bac4-c8d69be50e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"delta\").load(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta\"\n",
    ")\n",
    "df1.write.format(\"delta\").mode('overwrite').option().save(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta_Y\"\n",
    ")\n",
    "df1.write.delta(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta_Y\", overwriteSchema=True, mode=\"overwrite\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24d9f658-6753-4ca6-baa3-70b1bd6a80e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"custid int,name string,age double,dop date\"\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\",header=False,sep=\",\",mode='permissive',comment='#',quote=\"'\", escape=\"|\", dateFormat=\"yyyy-dd-MM\", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=False, multiLine=True)\n",
    "\n",
    "cleaned = df1.selectExpr(\n",
    "    \"regexp_replace(name, '\\\\\\\\', '~') as name\"\n",
    ")\n",
    "display(cleaned)\n",
    "# if we use multiline = true \n",
    "# display() - it shows as expanded text area\n",
    "# show() - it shows as single line with \\n\n",
    "cleaned.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8f6dea8-6e8a-4dd5-bddb-509f42a06711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "raw = spark.read.text(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\")\n",
    "\n",
    "cleaned = raw.selectExpr(\n",
    "    \"regexp_replace(value, '\\\\\\\\', '|') as value\"\n",
    ")\n",
    "\n",
    "cleaned.write.mode(\"overwrite\").text(\"/tmp/cleaned_csv\")\n",
    "df1 = spark.read.schema(struct1).csv(\n",
    "    \"/tmp/cleaned_csv\",\n",
    "    header=False,\n",
    "    sep=\",\",\n",
    "    escape=\"|\",\n",
    "    quote=\"'\",\n",
    "    mode=\"permissive\",\n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f716c7d-cad4-4dc6-9174-eecd831bc3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When to go for quote: If the data is having delimiter in it..\n",
    "#When to go for escape: If the data is having quote in it...\n",
    "\n",
    "struct1=\"custid int,name string,age double,dop date,corrupt_record string\"\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\",header=False,sep=\",\",mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\", escape=\"|\", dateFormat=\"yyyy-dd-MM\", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=False)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a7f72d-1621-45f1-aec4-c0e6531a1579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_bytes = b'caf\\xc3\\xa9'\n",
    "text = raw_bytes.decode(\"utf-8\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c1fb058f-5011-418b-b420-357f5534c4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"custid int,name string,age double,dop date\"\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\",header=False,sep=\",\",mode='permissive',comment='#',quote=\"'\", escape=\"|\", dateFormat=\"yyyy-dd-MM\", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=False, multiLine=True)\n",
    "\n",
    "cleaned = df1.selectExpr(\n",
    "    \"regexp_replace(value, '\\\\\\\\', '~') as value\"\n",
    ")\n",
    "display(df1)\n",
    "# if we use multiline = true \n",
    "# display() - it shows as expanded text area\n",
    "# show() - it shows as single line with \\n\n",
    "df1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5fada71-894d-4e23-9adb-8835ba16d5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "raw = spark.read.text(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\")\n",
    "\n",
    "cleaned = raw.selectExpr(\n",
    "    \"regexp_replace(value, '\\\\\\\\', '|') as value\"\n",
    ")\n",
    "\n",
    "cleaned.write.mode(\"overwrite\").text(\"/tmp/cleaned_csv\")\n",
    "df1 = spark.read.schema(struct1).csv(\n",
    "    \"/tmp/cleaned_csv\",\n",
    "    header=False,\n",
    "    sep=\",\",\n",
    "    escape=\"|\",\n",
    "    quote=\"'\",\n",
    "    mode=\"permissive\",\n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "639d4c48-3b35-420b-94a1-7ae7ea9d5eb5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/workspace/wd36schema/ingestion_volume/source/custsmodified\")\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\")\n",
    "display(cleanseddf)\n",
    "print(\"Total rows without firstname and lastname with null values\",len(cleanseddf.collect()))\n",
    "\n",
    "cleanseddf1=rawdf1.na.drop(how=\"all\")\n",
    "display(cleanseddf1)\n",
    "print(\"Total rows without firstname and lastname with null values\",len(cleanseddf1.collect()))\n",
    "\n",
    "print(\"any one row in the raw df with age null\")\n",
    "display(rawdf1.where(\"age is null\"))\n",
    "print(\"any one row in the cleansed df with age null\")\n",
    "display(cleanseddf.where(\"age is null\"))\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\",subset=[\"id\",\"age\"])\n",
    "print(\"any one row in the cleansed df with id or age null\")\n",
    "display(cleanseddf)\n",
    "cleanseddf=rawdf1.na.drop(how=\"all\",subset=[\"firstname\",\"lastname\"])\n",
    "print(\"any one row in the cleansed df with firstname and lastname is null\")\n",
    "print(\"Total rows without firstname and lastname with null values\",len(cleanseddf.collect()))\n",
    "display(cleanseddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd46dd71-31ee-4476-a231-7b2767c1d7c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cleanseddf=rawdf1.na.drop(how=\"any\")#how=\"any\" â†’ drop the row if any column has a null value\n",
    "\n",
    "print(\"any one row in the raw df with age null\")\n",
    "display(rawdf1.where(\"age is null\"))\n",
    "\n",
    "print(\"any one row in the cleansed df with age null\")\n",
    "display(cleanseddf.where(\"age is null\"))#any one column contains null will be cleaned\n",
    "\n",
    "cleanseddf=rawdf1.na.drop(how=\"any\",subset=[\"id\",\"age\"])#If we need CDE without nulls (Critical Data Elements/Significant columns) columns\n",
    "print(\"any one row in the cleansed df with id or age null\")\n",
    "display(cleanseddf)\n",
    "print(\"Total rows without id\",\"age with null values\",len(cleanseddf.collect()))\n",
    "\n",
    "cleanseddf=rawdf1.na.drop(how=\"all\",subset=[\"firstname\",\"lastname\"])#4000004,Gretchen,,66, \n",
    "print(\"any one row in the cleansed df with firstname and lastname is null\")\n",
    "print(\"Total rows without firstname and lastname with null values\",len(cleanseddf.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8e7edf7f-aa74-4257-a286-f5f22dc8a7c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(cleanseddf.where(\"age is null or id is null\"))#any\n",
    "display(cleanseddf.where(\"age is null and id is null\"))#and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ad72ae1-8cdd-4dce-975d-c809355a024c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data1 = [\n",
    "    {\"id\": 1, \"firstname\": \"John\", \"lastname\": \"Doe\"},\n",
    "    {\"id\": 2, \"firstname\": \"Jane\", \"lastname\": \"Smith\"}\n",
    "]\n",
    "\n",
    "data2 = [\n",
    "    {\"id\": 3, \"firstname\": \"Mike\", \"lastname\": \"Brown\", \"age\": 30},\n",
    "    {\"id\": 4, \"firstname\": \"Sara\", \"lastname\": \"Wilson\", \"age\": 25},\n",
    "    {\"id\": 5, \"firstname\": \"william\", \"lastname\": \"Wilson\", \"age\": 35}\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data1)\n",
    "df2 = spark.createDataFrame(data2)\n",
    "\n",
    "# Write JSON files\n",
    "df1.write.mode(\"overwrite\").json(\"/Volumes/sample_notebook/practice_workspace/volume1/file1\")\n",
    "df2.write.mode(\"overwrite\").json(\"/Volumes/sample_notebook/practice_workspace/volume1/file2\")\n",
    "\n",
    "json_df = spark.read.json(\"/Volumes/sample_notebook/practice_workspace/volume1/file*\")\n",
    "json_df.show()\n",
    "\n",
    "json_df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").saveAsTable(\"customer_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f61be167-ae31-46b7-b7bc-cf0b6bcef782",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(scrubbeddf3.where(\"id in ('4000001')\"))#before row level dedup\n",
    "dedupdf1=scrubbeddf3.distinct()#It will remove the row level duplicates\n",
    "display(dedupdf1.where(\"id in ('4000001')\"))\n",
    "\n",
    "print(\"non prioritized deduplication, just remove the duplicates retaining only the first row\")\n",
    "display(dedupdf1.coalesce(1).where(\"id in ('4000003')\"))#before col level dedup\n",
    "dedupdf2=dedupdf1.coalesce(1).dropDuplicates(subset=[\"id\"])#It will remove the column level duplicates (retaining the first row in the dataframe)\n",
    "display(dedupdf2.where(\"id in ('4000003')\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1eb0f3bc-aee3-4dcf-941d-6e909a87bb41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap,col\n",
    "#withColumn(\"stringcolumnname to add in the df\",lit('hardcoded')/initcap(col(\"colname\")))\n",
    "standarddf1=dedupdf2.withColumn(\"sourcesystem\",lit(\"Retail\"))#SparkSQL - DSL(FBP)\n",
    "display(standarddf1.limit(20))"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5473497148329557,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Practice_Notebook_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
