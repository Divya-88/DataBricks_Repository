{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d16855dd-6682-4704-bc4c-2b8b49c6bc1f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=True,inferSchema=True,sep=\",\")\n",
    "display(df_cust)\n",
    "\n",
    "df_cust.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/\", mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dcb3061a-afc8-475d-99ae-6fd7ae8d909c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE telecom_catalog_assign.landing_zone.customer_managed1 (\n",
    "    customer_id INT,\n",
    "    name STRING,\n",
    "    age INT,\n",
    "    city STRING,\n",
    "    plan STRING\n",
    ")\n",
    "USING DELTA;\n",
    "\n",
    "SELECT * FROM telecom_catalog_assign.landing_zone.customer_managed1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d418a04-53bb-4dc5-a82e-ea9281932fb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", header=True, inferSchema=True).toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "df_cust.write.saveAsTable(\"telecom_catalog_assign.landing_zone.customer_managed2\")\n",
    "df_cust.write.insertInto(\n",
    "    \"telecom_catalog_assign.landing_zone.customer_managed2\",\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "display(spark.sql(\"SELECT * FROM telecom_catalog_assign.landing_zone.customer_managed2\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ce9af3b-13b5-4f9f-a941-f214cccf73d6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE telecom_catalog_assign.landing_zone.customer_managed;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0d1fb8e9-63fa-4084-95ad-80711a4753ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1: Write customer data using insertInto() in a new table and find the behavior\n",
    "#table created for customer\n",
    "read_customer_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", sep=\",\").toDF(\"id\",\"name\",\"age\",\"city\",\"sim_type\") \n",
    "read_customer_df.write.saveAsTable(\"telecom_catalog_assign.landing_zone.custtbl\",mode='overwrite')\n",
    "#insert into customer table already created\n",
    "dlt_tb = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/custs.csv\").toDF(\"id\", \"name\", \"age\", \"city\", \"sim_type\")\n",
    "dlt_tb.write.insertInto(\"telecom_catalog_assign.landing_zone.custtbl\")\n",
    "display(spark.sql(\"select * from telecom_catalog_assign.landing_zone.custtbl\"))\n",
    "\n",
    "#2:Write usage data using insertTable() with overwrite mode\n",
    "read_usage = spark.read.options(header=\"True\",sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_new.csv\") \n",
    "read_usage.write.insertInto(\"telecom_catalog_assign.landing_zone.usagetbl\",overwrite=True)\n",
    "spark.sql(\"select * from telecom_catalog_assign.landing_zone.usagetbl\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0fdb596d-9a2d-44de-aa76-fd9e0f1f0aaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaOverwriteExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (108, \"John\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (109,\"Jane\",20,\"Delhi\",\"PREPAID\"),\n",
    "    (110,\"Alice\",25,\"Pune\",\"POSTPAID\"),\n",
    "    (111,\"Bob\",30,\"Bangalore\",\"PREPAID\"),\n",
    "\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"city\", \"plan\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Path to Delta table\n",
    "delta_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/\"\n",
    "\n",
    "# Write DataFrame in Delta format using overwrite mode\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "spark.read.format(\"delta\").load(delta_path).count()\n",
    "\n",
    "print(\"Data written to Delta table with overwrite mode successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "be313fb5-5ca1-4ea7-a702-988c82297cf9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "data = [\n",
    "    (101, \"John\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (103,\"Jane\",20,\"Delhi\",\"PREPAID\"),\n",
    "    (102,\"Alice\",25,\"Pune\",\"POSTPAID\"),\n",
    "    (107,\"Bob\",30,\"Bangalore\",\"PREPAID\"),\n",
    "    (106,\"Bob\",30,\"Bangalore\",\"PREPAID\")\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"age\", \"city\", \"plan\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer_practice/\")\n",
    "df1 = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer_practice/\")\n",
    "display(df1)\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5929b88-468f-4dcc-a47b-fae93bb4e632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DeltaOverwriteExample\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Sample DataFrame\n",
    "data = [\n",
    "    (108, \"John\",52,\"Mumbai\",\"POSTPAID\"),\n",
    "    (109,\"Jane\",20,\"Delhi\",\"PREPAID\"),\n",
    "    (110,\"Alice\",25,\"Pune\",\"POSTPAID\"),\n",
    "    (111,\"Bob\",30,\"Bangalore\",\"PREPAID\"),\n",
    "]\n",
    "\n",
    "columns = [\"id\", \"name\", \"amt\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Path to Delta table\n",
    "delta_path = \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\"\n",
    "\n",
    "# Write DataFrame in Delta format using overwrite mode\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(delta_path)\n",
    "\n",
    "# print(\"Data written to Delta table with overwrite mode successfully.\")\n",
    "# df.write.format(\"delta\").option(\"mergeSchema\", \"true\") \\\n",
    "#     .mode(\"overwrite\") \\\n",
    "#     .save(delta_path)\n",
    "\n",
    "print(\"Data written to Delta table with overwrite mode successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3cf8bf2-f95f-4915-90d4-af96b81f1423",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/X.c000.snappy.parquet\")\n",
    "# df1.show()\n",
    "df1.write.format(\"delta\").mode(\"overwrite\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/Y.c000.snappy.parquet\")\n",
    "df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77c3703a-eed8-4df5-bac4-c8d69be50e1a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df1 = spark.read.format(\"delta\").load(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta\"\n",
    ")\n",
    "df1.write.format(\"delta\").mode('overwrite').option().save(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta_Y\"\n",
    ")\n",
    "df1.write.delta(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta_Y\", overwriteSchema=True, mode=\"overwrite\")\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d9f658-6753-4ca6-baa3-70b1bd6a80e6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"custid int,name string,age double,dop date\"\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\",header=False,sep=\",\",mode='permissive',comment='#',quote=\"'\", escape=\"|\", dateFormat=\"yyyy-dd-MM\", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=False, multiLine=True)\n",
    "\n",
    "cleaned = df1.selectExpr(\n",
    "    \"regexp_replace(value, '\\\\\\\\', '~') as value\"\n",
    ")\n",
    "display(df1)\n",
    "# if we use multiline = true \n",
    "# display() - it shows as expanded text area\n",
    "# show() - it shows as single line with \\n\n",
    "df1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8f6dea8-6e8a-4dd5-bddb-509f42a06711",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "raw = spark.read.text(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\")\n",
    "\n",
    "cleaned = raw.selectExpr(\n",
    "    \"regexp_replace(value, '\\\\\\\\', '|') as value\"\n",
    ")\n",
    "\n",
    "cleaned.write.mode(\"overwrite\").text(\"/tmp/cleaned_csv\")\n",
    "df1 = spark.read.schema(struct1).csv(\n",
    "    \"/tmp/cleaned_csv\",\n",
    "    header=False,\n",
    "    sep=\",\",\n",
    "    escape=\"|\",\n",
    "    quote=\"'\",\n",
    "    mode=\"permissive\",\n",
    "    multiLine=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6f716c7d-cad4-4dc6-9174-eecd831bc3d9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#When to go for quote: If the data is having delimiter in it..\n",
    "#When to go for escape: If the data is having quote in it...\n",
    "\n",
    "struct1=\"custid int,name string,age double,dop date,corrupt_record string\"\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\",header=False,sep=\",\",mode='permissive',comment='#',columnNameOfCorruptRecord=\"corrupt_record\",quote=\"'\", escape=\"|\", dateFormat=\"yyyy-dd-MM\", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=False)\n",
    "\n",
    "display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18a7f72d-1621-45f1-aec4-c0e6531a1579",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_bytes = b'caf\\xc3\\xa9'\n",
    "text = raw_bytes.decode(\"utf-8\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c1fb058f-5011-418b-b420-357f5534c4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "struct1=\"custid int,name string,age double,dop date\"\n",
    "\n",
    "df1=spark.read.schema(struct1).csv(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\",header=False,sep=\",\",mode='permissive',comment='#',quote=\"'\", escape=\"|\", dateFormat=\"yyyy-dd-MM\", ignoreLeadingWhiteSpace=True, ignoreTrailingWhiteSpace=False, multiLine=True)\n",
    "\n",
    "cleaned = df1.selectExpr(\n",
    "    \"regexp_replace(value, '\\\\\\\\', '~') as value\"\n",
    ")\n",
    "display(df1)\n",
    "# if we use multiline = true \n",
    "# display() - it shows as expanded text area\n",
    "# show() - it shows as single line with \\n\n",
    "df1.show(10,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5fada71-894d-4e23-9adb-8835ba16d5ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "raw = spark.read.text(\"/Volumes/workspace/wd36schema/ingestion_volume/malformeddata1.txt\")\n",
    "\n",
    "cleaned = raw.selectExpr(\n",
    "    \"regexp_replace(value, '\\\\\\\\', '|') as value\"\n",
    ")\n",
    "\n",
    "cleaned.write.mode(\"overwrite\").text(\"/tmp/cleaned_csv\")\n",
    "df1 = spark.read.schema(struct1).csv(\n",
    "    \"/tmp/cleaned_csv\",\n",
    "    header=False,\n",
    "    sep=\",\",\n",
    "    escape=\"|\",\n",
    "    quote=\"'\",\n",
    "    mode=\"permissive\",\n",
    "    multiLine=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8175749884540104,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Practice_Notebook_1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
