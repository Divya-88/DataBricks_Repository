{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e56e4759-b851-47b3-8705-37b05b93ff7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain Write Ops Assignment - Building Datalake & Lakehouse\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5837af8d-98f0-46a9-9826-31c8258b7cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Spark Write Operations using \n",
    "- csv, json, orc, parquet, delta, saveAsTable, insertInto, xml with different write mode, header and sep options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "184e8b3d-e7df-48a8-b8ca-6d976a058c8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##6. Write Operations (Data Conversion/Schema migration) – CSV Format Usecases\n",
    "1. Write customer data into CSV format using overwrite mode\n",
    "2. Write usage data into CSV format using append mode\n",
    "3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca242f3d-d9b9-440c-9207-abcc3745c698",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Write customer data into CSV format using overwrite mode\n",
    "#6.1. Write customer data into CSV format using overwrite mode\n",
    "df_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",header=True,inferSchema=True,sep=\",\")\n",
    "\n",
    "df_cust.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_csv\",header=True,mode=\"overwrite\")\n",
    "display(df_cust)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "76b337b5-ae94-4878-a081-5fb7e4fc9546",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.2. Write usage data into CSV format using append mode\n",
    "usage_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", header=True, inferSchema=True, sep=\"\\t\")\n",
    "\n",
    "usage_df.write.csv(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_out_csv\",header=True, mode=\"append\")\n",
    "display(usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d90eaf5c-cb60-467f-9301-65ad714ae542",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.3. Write tower data into CSV format with header enabled and custom separator (|)\n",
    "tower_final_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower_all_data/tower_all_data_src/tower_all.csv\",header = True, sep = '|')\n",
    "\n",
    "tower_final_df.write.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower_all_data/tower_all_data_tgt/\",header=True,mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "da363f64-6efb-4f37-bafb-756dccf11ad1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#6.4. Read the tower data in a dataframe and show only 5 rows.\n",
    "df_tower_read = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower_all_data/tower_all_data_tgt/\",sep='|',header=True)\n",
    "df_tower_read.show(5)\n",
    "\n",
    "#6.5. Download the file into local from the catalog volume location and see the data of any of the above files opening in a notepad++\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b982d3be-4d68-467f-8f56-86a2cb403d7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##7. Write Operations (Data Conversion/Schema migration)– JSON Format Usecases\n",
    "1. Write customer data into JSON format using overwrite mode\n",
    "2. Write usage data into JSON format using append mode and snappy compression format\n",
    "3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "4. Read the tower data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d56e4be-1d45-4efc-a3f9-0980785a588b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#7.1. Write customer data into JSON format using overwrite mode\n",
    "df_cust.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_json\", mode=\"overwrite\")\n",
    "display(df_cust)\n",
    "\n",
    "#7.2. Write usage data into JSON format using append mode and snappy compression format\n",
    "usage_df.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_out_json\", mode=\"append\", compression=\"snappy\")\n",
    "display(usage_df)\n",
    "\n",
    "#7.3. Write tower data into JSON format using ignore mode and observe the behavior of this mode\n",
    "tower_final_df.write.json(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_json\", mode=\"ignore\")\n",
    "display(tower_final_df)\n",
    "\n",
    "#7.4. Read the tower data in a dataframe and show only 5 rows.\n",
    "tower_df3 = spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_json\")\n",
    "display(tower_df3.limit(5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "af44e6ec-38a1-43f3-b9b3-6038068294cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##8. Write Operations (Data Conversion/Schema migration) – Parquet Format Usecases\n",
    "1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "2. Write usage data into Parquet format using error mode\n",
    "3. Write tower data into Parquet format with gzip compression option\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "c50c7fa5-2487-4709-bcba-cdb23bdac809",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 8.1. Write customer data into Parquet format using overwrite mode and in a gzip format\n",
    "df_cust.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_json\", mode=\"overwrite\", compression=\"gzip\")\n",
    "\n",
    "#8.2. Write usage data into Parquet format using error mode\n",
    "usage_df.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_out_parquet\", mode=\"error\")\n",
    "\n",
    "#8.3. Write tower data into Parquet format with gzip compression option\n",
    "tower_final_df.write.parquet(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_parquet\", mode=\"overwrite\",compression=\"gzip\")\n",
    "\n",
    "#8.4. Read the usage data in a dataframe and show only 5 rows.\n",
    "tower_df3 = spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_json\")\n",
    "display(tower_df3.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b0697ee-86ff-415c-815b-e08d3c1565f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##9. Write Operations (Data Conversion/Schema migration) – Orc Format Usecases\n",
    "1. Write customer data into ORC format using overwrite mode\n",
    "2. Write usage data into ORC format using append mode\n",
    "3. Write tower data into ORC format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41b7908b-111b-49a9-98eb-df0bd2d45701",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 9.1. Write customer data into ORC format using overwrite mode\n",
    "df_cust.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_orc\", mode=\"overwrite\")\n",
    "\n",
    "#9.2. Write usage data into ORC format using append mode\n",
    "usage_df.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_out_orc\", mode=\"append\")\n",
    "\n",
    "#9.3. Write tower data into ORC format and see the output file structure\n",
    "tower_final_df.write.orc(path=\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_orc\", mode=\"overwrite\")\n",
    "# List files in the output directory\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_orc\")\n",
    "\n",
    "#9.4. Read the usage data in a dataframe and show only 5 rows.\n",
    "tower_df3 = spark.read.json(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_json\")\n",
    "display(tower_df3.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95396ab5-a72c-4f1d-8a8c-e892a4a5f279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##10. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "1. Write customer data into Delta format using overwrite mode\n",
    "2. Write usage data into Delta format using append mode\n",
    "3. Write tower data into Delta format and see the output file structure\n",
    "4. Read the usage data in a dataframe and show only 5 rows.\n",
    "5. Download the file into local harddisk from the catalog volume location and see the data of any of the above files opening in a notepad++.\n",
    "6. Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16075a6a-a6a2-4fbd-9729-21ccfbfb16ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 10.1. Write Operations (Data Conversion/Schema migration) – Delta Format Usecases\n",
    "df_cust.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/\", mode=\"overwrite\")\n",
    "\n",
    "# df_cust.write.format(\"delta\").option(\"mergeSchema\", \"true\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_delta/\", mode=\"overwrite\")\n",
    "# In Databricks (and Delta Lake), the option mergeSchema = true is used when writing data to a Delta table and you want to automatically update the table’s schema if the incoming DataFrame has new columns that don’t exist in the table.\n",
    "# 1. Basic Concept\n",
    "# By default, if you try to write a DataFrame with columns not present in the Delta table, you’ll get an error.\n",
    "# Setting mergeSchema = true allows automatic schema evolution, so new columns are added to the table without failing the write.\n",
    "\n",
    "# 10.2. Write usage data into Delta format using append mode\n",
    "usage_df.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_out_delta\", mode=\"append\")\n",
    "\n",
    "# 10.3. Write tower data into Delta format and see the output file structure\n",
    "tower_final_df.write.format(\"delta\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_delta\", mode=\"overwrite\")\n",
    "dbutils.fs.ls(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_out_delta\")\n",
    "\n",
    "# 10.4. Read the usage data in a dataframe and show only 5 rows.\n",
    "usage_df_delta = spark.read.format(\"delta\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_out_delta\")\n",
    "display(usage_df_delta.limit(5))\n",
    "\n",
    "# 10.6.Compare the parquet location and delta location and try to understand what is the differentiating factor, as both are parquet files only.\n",
    "# both are parquet files, but delta has a _delta_log folder which contains the transaction logs and metadata for the table.\n",
    "# The _delta_log folder contains information about the changes made to the table, including the version of the table, the timestamp of the change, and the details of the change itself.\n",
    "# This information is used by Delta Lake to ensure data integrity and to recover from failures.\n",
    "# The _delta_log folder also contains information about the schema of the table, including the names and data types of the columns.\n",
    "# This information is used by Delta Lake to ensure that the schema of the table is consistent across all versions of the table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "025d35ab-29fb-47e7-9b42-602ffad6efa8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##11. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using saveAsTable() as a managed table\n",
    "2. Write usage data using saveAsTable() with overwrite mode\n",
    "3. Drop the managed table and verify data removal\n",
    "4. Go and check the table overview and realize it is in delta format in the Catalog.\n",
    "5. Use spark.read.sql to write some simple queries on the above tables created.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0d9ce11b-e6bb-4821-ad55-326cd77a8a68",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 11.1. Write customer data using saveAsTable() as a managed table\n",
    "df_cust.write.saveAsTable(\"telecom_catalog_assign.landing_zone.customer_managed\", mode=\"overwrite\")\n",
    "# df_cust.write.option(\"mergeSchema\", \"true\").saveAsTable(\"telecom_catalog_assign.landing_zone.customer_managed\", mode=\"overwrite\")\n",
    "# or\n",
    "# df_cust.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"telecom_catalog_assign.landing_zone.customer_managed\")\n",
    "\n",
    "# 11.2. Write usage data using saveAsTable() with overwrite mode\n",
    "usage_df.write.saveAsTable(\"telecom_catalog_assign.landing_zone.usage_managed\", mode=\"overwrite\")\n",
    "\n",
    "# 11.3. Drop the managed table and verify data removal - using python\n",
    "display(spark.sql(\"SHOW TABLES IN telecom_catalog_assign.landing_zone\")) # display before dropping\n",
    "\n",
    "spark.sql(\"DROP TABLE IF EXISTS telecom_catalog_assign.landing_zone.cust_other_del\")\n",
    "\n",
    "# Verify data removal by listing tables\n",
    "display(spark.sql(\"SHOW TABLES IN telecom_catalog_assign.landing_zone\")) # display after dropping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "55b6bb50-85a7-40a0-ac72-2e82da54992b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DROP TABLE telecom_catalog_assign.landing_zone.cust_other_del;\n",
    "-- 11.3. Drop the managed table and verify data removal - using sql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "dd659c5c-14a4-47f3-b87c-190d6208a8f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"SELECT * FROM telecom_catalog_assign.landing_zone.customer_managed\").head(2))\n",
    "spark.sql(\"SELECT * FROM telecom_catalog_assign.landing_zone.customer_managed\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e421f4c-4831-4421-a419-0b5ee055d3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##12. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data using insertInto() in a new table and find the behavior\n",
    "2. Write usage data using insertTable() with overwrite mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56499fde-385f-4e15-8751-d01d73620166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 12.1 Write customer data using insertInto() in a new table and find the behavior\n",
    "df_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", header=True, inferSchema=True,sep=\",\").toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "df_cust.write.saveAsTable(\"telecom_catalog_assign.landing_zone.customer_managed_other\", mode=\"overwrite\")\n",
    "display(df_cust)\n",
    "# df_cust.write.insertInto(\"telecom_catalog_assign.landing_zone.customer_managed\", overwrite=True)]\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\").toDF([\"customer_id\", \"name\", \"age\", \"city\", \"plan\"])\n",
    "\n",
    "delta_cust1 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/customer1.csv\", inferSchema=True,sep=\",\").toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "\n",
    "display(delta_cust1)\n",
    "delta_cust1.write.insertInto(\"telecom_catalog_assign.landing_zone.customer_managed_other\", overwrite=True)\n",
    "\n",
    "# delta_cust1.write.insertInto(\"telecom_catalog_assign.landing_zone.customer_managed_other\", overwrite=True) - insertInto() defaults to append mode, insertInto() supports overwrite ONLY in Spark SQL, NOT reliably in the DataFrame API\n",
    "#Schema should match for 2 file datasets\n",
    "\n",
    "display(spark.sql(\"SELECT * FROM telecom_catalog_assign.landing_zone.customer_managed_other\"))\n",
    "\n",
    "# 12.2. Write usage data using insertTable() with overwrite mode\n",
    "usage_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", header=True, inferSchema=True, sep=\"\\t\")\n",
    "usage_df.write.insertInto(\"telecom_catalog_assign.landing_zone.usage_managed\", overwrite=True)\n",
    "display(spark.sql(\"SELECT * FROM telecom_catalog_assign.landing_zone.usage_managed\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d797466-f418-4e57-a877-134e673975bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#12.1. Write customer data using insertInto() in a new table and find the behavior\n",
    "\n",
    "#12.2. Write usage data using insertTable() with overwrite mode\n",
    "usage_df.write.insertTable(\"telecom_catalog_assign.landing_zone.usage_managed\", mode=\"overwrite\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b178e0c7-c848-4a63-bbd9-49fd0b30d928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##13. Write Operations (Lakehouse Usecases) – Delta table Usecases\n",
    "1. Write customer data into XML format using rowTag as cust\n",
    "2. Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "3. Download the xml data and open the file in notepad++ and see how the xml file looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2c6b760d-a5e1-4be9-9605-5edb365986ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 13.1 Write customer data into XML format using rowTag as cust\n",
    "df_cust = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\", header=True, inferSchema=True).toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan\")\n",
    "display(df_cust)\n",
    "\n",
    "df_cust.write.format(\"xml\").mode(\"overwrite\").option(\"rowTag\", \"cust\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer_out_xml\")\n",
    "\n",
    "# 13.2 Write usage data into XML format using overwrite mode with the rowTag as usage\n",
    "usage_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\", header=True, inferSchema=True, sep=\"\\t\")\n",
    "#actual data format used [space] as delimiter so to read the data as table give sep\n",
    "display(usage_df)\n",
    "usage_df.write.format(\"xml\").mode(\"overwrite\").option(\"rowTag\", \"usage\").save(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage_out_xml\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c28b03a4-403f-411e-86c3-2eefe809e2cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##14. Compare all the downloaded files (csv, json, orc, parquet, delta and xml) \n",
    "1. Capture the size occupied between all of these file formats and list the formats below based on the order of size from small to big."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f410167c-6412-465f-ba94-217d6d3f9cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###15. Try to do permutation and combination of performing Schema Migration & Data Conversion operations like...\n",
    "1. Read any one of the above orc data in a dataframe and write it to dbfs in a parquet format\n",
    "2. Read any one of the above parquet data in a dataframe and write it to dbfs in a delta format\n",
    "3. Read any one of the above delta data in a dataframe and write it to dbfs in a xml format\n",
    "4. Read any one of the above delta table in a dataframe and write it to dbfs in a json format\n",
    "5. Read any one of the above delta table in a dataframe and write it to another table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "97be8f93-c98e-4876-b36e-c690198af7f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##15. Do a final exercise of defining one/two liner of... \n",
    "1. When to use/benifits csv\n",
    "2. When to use/benifits json\n",
    "3. When to use/benifit orc\n",
    "4. When to use/benifit parquet\n",
    "5. When to use/benifit delta\n",
    "6. When to use/benifit xml\n",
    "7. When to use/benifit delta tables\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8175749884540092,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "write_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
