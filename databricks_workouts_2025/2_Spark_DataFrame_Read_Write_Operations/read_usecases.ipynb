{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e56e4759-b851-47b3-8705-37b05b93ff7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Telecom Domain Read & Write Ops Assignment - Building Datalake & Lakehouse\n",
    "This notebook contains assignments to practice Spark read options and Databricks volumes. <br>\n",
    "Sections: Sample data creation, Catalog & Volume creation, Copying data into Volumes, Path glob/recursive reads, toDF() column renaming variants, inferSchema/header/separator experiments, and exercises.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "841c7ed8-ef18-486a-8187-07685e499b84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "![](https://fplogoimages.withfloats.com/actual/68009c3a43430aff8a30419d.png)\n",
    "![](https://theciotimes.com/wp-content/uploads/2021/03/TELECOM1.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d4aa0a44-8cd6-41cf-921d-abb5ff67615b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0b67823-2e4e-45e2-aa25-80550a3ac580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##1. Write SQL statements to create:\n",
    "1. A catalog named telecom_catalog_assign\n",
    "2. A schema landing_zone\n",
    "3. A volume landing_vol\n",
    "4. Using dbutils.fs.mkdirs, create folders:<br>\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\n",
    "/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "5. Explain the difference between (Just google and understand why we are going for volume concept for prod ready systems):<br>\n",
    "a. Volume vs DBFS/FileStore<br>\n",
    "b. Why production teams prefer Volumes for regulated data<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88b482a1-1151-427b-8317-daa69d956529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--1. Write SQL statements to create:\n",
    "--1.1. A catalog named telecom_catalog_assign\n",
    "CREATE CATALOG IF NOT EXISTS telecom_catalog_assign;\n",
    "\n",
    "--1.2. A schema landing_zone\n",
    "CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone;\n",
    "\n",
    "--1.3. A volume landing_vol\n",
    "CREATE VOLUME IF NOT EXISTS telecom_catalog_assign.landing_zone.landing_vol;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba0a658d-4f22-4fca-a071-78f42b2e7637",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "--drop catalag even data is present inside - for reference\n",
    "--DROP CATALOG IF EXISTS telecom_catalog_assign CASCADE;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a609283-e553-4c18-ae12-46384689f70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1.4. Using dbutils.fs.mkdirs, create folders:\n",
    "# /Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/ \n",
    "# /Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/ \n",
    "# /Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\n",
    "\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage\")\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e417945c-3038-42f8-b265-73e894fcb2a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# # We can create multiple folders in one go by using a loop, instead of writing dbutils.fs.mkdirs() one by one.\n",
    "# folders = ['/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1 ','/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region2']\n",
    "\n",
    "# for folder in folders:\n",
    "#   dbutils.fs.mkdirs(folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "492b4db9-89fa-46b8-9dea-022511299b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Explain the difference between dbfs and volume in databricks:\n",
    "##### 1. DBFS (Databricks File System)<br>\n",
    "- A legacy abstraction layer over cloud object storage (ADLS, S3, GCS).\n",
    "- Makes cloud storage look like a local file system.\n",
    "- Tied to the workspace, not Unity Catalog\n",
    "- Limited governance and security\n",
    "- Often accessed using dbutils.fs\n",
    "- Common in older Databricks projects\n",
    "\n",
    "##### 2. Volumes (Unity Catalog Volumes)\n",
    "- A modern, governed storage abstraction introduced with Unity Catalog. \n",
    "- Provides secure, managed access to cloud storage.\n",
    "- Governed by Unity Catalog. \n",
    "- Fine-grained permissions (READ, WRITE). \n",
    "- Works seamlessly with SQL, Python, Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "26d8bd3d-b575-448b-ae22-8173d15ca671",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25d58bab-9e5c-4f50-be66-64f91913edd4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "5002|104|TWR06|-75|2025-01-10 11:01:12\n",
    "5003|104|TWR02|-80|2025-01-10 11:01:12\n",
    "5005|104|TWR03|-75|2025-01-10 11:01:12\n",
    "5006|104|TWR04|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9540d2e2-2562-4be7-897f-0a7d57adaa72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write code to copy the above datasets into your created Volume folders:\n",
    "Customer → /Volumes/.../customer/\n",
    "Usage → /Volumes/.../usage/\n",
    "Tower (region-based) → /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "353617bb-f699-4024-8c58-197447a36fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.1. Write code to copy the above datasets into your created Volume folders:\n",
    "# dbutils.fs.put(path: str, contents: str[file content or variable], overwrite: bool)\n",
    "dbutils.fs.put('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv', customer_csv, True)\n",
    "\n",
    "dbutils.fs.put('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv', usage_tsv, True)\n",
    "\n",
    "dbutils.fs.put(f\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/ericsson/tower_region1_ericsson.csv\", tower_logs_region1,overwrite = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "9d3895b4-64ad-4fea-a66e-4e6ee42fb71f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 2.2. Write a command to validate whether files were successfully copied\n",
    "\n",
    "print(dbutils.fs.ls('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/'))\n",
    "print(dbutils.fs.ls('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/'))\n",
    "print(dbutils.fs.ls('/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8767735b-24d3-428a-ad12-ae821903e2ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0b6e89e-ce74-4808-b450-8bde05c899a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3.1\n",
    "Tower_log_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\", header=True, inferSchema=True, pathGlobFilter=\"*.csv\", recursiveFileLookup=True, sep=\"|\")\n",
    "display(Tower_log_df)\n",
    "\n",
    "# spark.read.format(csv).option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"recursiveFileLookup\",\"true\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\").display()\n",
    "\n",
    "#3.2\n",
    "tower_log_df1 = spark.read.csv([\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\", \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower_all_data/\"], header=True, inferSchema=True, recursiveFileLookup=True, pathGlobFilter=\"*.csv\", sep=\"|\")\n",
    "display(tower_log_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f7147c1-5d58-47e1-84fe-7ebd26a217b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "30c1c5f2-d8e4-418f-9d03-9711d036a589",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4.1\n",
    "customer_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\", header=True, inferSchema=True).printSchema()\n",
    "display(customer_df)\n",
    "\n",
    "#4.2\n",
    "#When Header is false and inferschema is false data format is taken as string, \n",
    "#When Header is true and inferschema is true is data format is taken as data format given in data file\n",
    "#when either header or inferschema is false, data format is taken as string\n",
    "\n",
    "# 4.3. How schema inference handled “abc” in age?\n",
    "# Age contains both integer and string value so it is considerign the age column as string[string + integer = string]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15d8dad0-bc63-47f1-9a90-72837cba6c4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ab255bb4-10c0-476f-9313-ae36d168512c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5.1\n",
    "dbutils.fs.rm(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/\", True)\n",
    "#custid,name,age,city,plan\n",
    "customer1_data = '''\n",
    "1,\"divya\",20,\"bangalore\",postpaid\n",
    "2,\"anu\",24,\"chennai\",postpaid\n",
    "3,\"sita\",25,\"hyderabad\",postpaid\n",
    "'''\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/\")\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/customer1.csv\", customer1_data, True)\n",
    "\n",
    "customer_df1 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/customer1.csv\",header=False, inferSchema=True).toDF(\"customer_id\",\"user_name\",\"age\",\"city\",\"plan\")\n",
    "#if we user todf() function no need to use of header = true\n",
    "# display(customer_df1)\n",
    "\n",
    "customer_df1.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/customer_csv_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e3005d6-6fa3-4634-bb38-993e60b1784c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5.2\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType\n",
    "usage_data_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"voice_mins\", IntegerType(), False),\n",
    "    StructField(\"data_mb\", IntegerType(), False),\n",
    "    StructField(\"sms_count\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "usage_df = spark.read.schema(usage_data_schema).options(header=\"True\", sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "#while using schema we need to use options() function\n",
    "\n",
    "display(usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7c3b9577-a526-40df-be24-ee0fc411cfff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5.3\n",
    "usage_data_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", LongType(), False),\n",
    "    StructField(\"tower_id\", StringType(), False),\n",
    "    StructField(\"signal_strength\", LongType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "tower_df1 = spark.read.schema(usage_data_schema).options(header=\"True\", sep=\"|\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\")\n",
    "display(tower_df1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 6305217924282347,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_usecases",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
