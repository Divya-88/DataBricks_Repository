{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d0f0b9e5-3f37-4d2d-b2d7-6bbaeed93191",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "3. Directory Read Use Cases\n",
    "Read all tower logs using: Path glob filter (example: *.csv) Multiple paths input Recursive lookup\n",
    "\n",
    "Demonstrate these 3 reads separately: Using pathGlobFilter Using list of paths in spark.read.csv([path1, path2]) Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1ba25103-5988-457d-85e8-769f275ad2f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "Tower_log_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\", header=True, inferSchema=True, pathGlobFilter=\"*.csv\", recursiveFileLookup=True, sep=\"|\")\n",
    "display(Tower_log_df)\n",
    "\n",
    "# spark.read.format(csv).option(\"header\", \"true\").option(\"inferSchema\", \"true\").option(\"recursiveFileLookup\",\"true\").load(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower\").display()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "13e797d7-11aa-4bdf-b6c3-eb5a3068ac31",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766244011283}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tower_log_df1 = spark.read.csv([\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\", \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower_all_data/\"], header=True, inferSchema=True, recursiveFileLookup=True, pathGlobFilter=\"*.csv\", sep=\"|\")\n",
    "display(tower_log_df1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40ccf845-bb56-4861-9739-f340fb3713bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled “abc” in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "379c7332-ea72-4f01-8c37-004fb18dc954",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer\", header=True, inferSchema=True).printSchema()\n",
    "display(customer_df)\n",
    "\n",
    "#When Header is false and inferschema is false data format is taken as string, \n",
    "#When Header is true and inferschema is true is data format is taken as data format given in data file\n",
    "#when either header or inferschema is false, data format is taken as string\n",
    "\n",
    "# 3. How schema inference handled “abc” in age?\n",
    "# Age contains both integer and string value so it is considerign the age column as string[string + integer = string]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1964fe11-9ce8-4098-bd32-6c6c80e8116e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "1. Apply column names using string using toDF function for customer data\n",
    "2. Apply column names and datatype using the schema function for usage data\n",
    "3. Apply column names and datatype using the StructType with IntegerType, StringType, TimestampType and other classes for towers data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c04d2b60-30ef-4f30-9ff9-cde412dc63c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.rm(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/\", True)\n",
    "#custid,name,age,city,plan\n",
    "customer1_data = '''\n",
    "1,\"divya\",20,\"bangalore\",postpaid\n",
    "2,\"anu\",24,\"chennai\",postpaid\n",
    "3,\"sita\",25,\"hyderabad\",postpaid\n",
    "'''\n",
    "dbutils.fs.mkdirs(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/\")\n",
    "dbutils.fs.put(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/customer1.csv\", customer1_data, True)\n",
    "\n",
    "customer_df1 = spark.read.csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/customer1.csv\",header=False, inferSchema=True).toDF(\"customer_id\",\"user_name\",\"age\",\"city\",\"plan\")\n",
    "#if we user todf() function no need to use of header = true\n",
    "# display(customer_df1)\n",
    "\n",
    "customer_df1.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer1/customer_csv_folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "7b190255-1a5e-4fd7-b37b-b80b53114443",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, DoubleType, TimestampType\n",
    "usage_data_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), False),\n",
    "    StructField(\"voice_mins\", IntegerType(), False),\n",
    "    StructField(\"data_mb\", IntegerType(), False),\n",
    "    StructField(\"sms_count\", IntegerType(), False),\n",
    "])\n",
    "\n",
    "usage_df = spark.read.schema(usage_data_schema).options(header=\"True\", sep=\"\\t\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.csv\")\n",
    "#while using schema we need to use options() function\n",
    "\n",
    "display(usage_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "54496422-5ade-401e-9883-910aefc506e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usage_data_schema = StructType([\n",
    "    StructField(\"event_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", LongType(), False),\n",
    "    StructField(\"tower_id\", StringType(), False),\n",
    "    StructField(\"signal_strength\", LongType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False)\n",
    "])\n",
    "\n",
    "tower_df1 = spark.read.schema(usage_data_schema).options(header=\"True\", sep=\"|\").csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/region1/tower_logs_region1.csv\")\n",
    "display(tower_df1)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Read_Usecase_solution",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
